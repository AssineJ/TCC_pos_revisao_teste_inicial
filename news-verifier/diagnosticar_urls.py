from modules.searcher import buscar_noticias
from modules.scraper import scrape_noticias
from modules.nlp_processor import processar_texto

print("=" * 70)
print("ğŸ” DIAGNÃ“STICO DE URLs TRUNCADAS")
print("=" * 70)
print()

# Usar a notÃ­cia problemÃ¡tica
texto_teste = """
O Supremo Tribunal Federal teve apenas trÃªs mulheres ministras em sua 
histÃ³ria e movimentos sociais comeÃ§am a cobrar do presidente Lula pela 
sucessÃ£o de Barroso. A discussÃ£o sobre diversidade de gÃªnero no STF 
ganhou forÃ§a com a aproximaÃ§Ã£o da aposentadoria do ministro.
"""

print("ETAPA 1: Processamento NLP")
print("-" * 70)
resultado_nlp = processar_texto(texto_teste)
print(f"Query: {resultado_nlp['query_busca']}")
print()

print("ETAPA 2: Busca nas fontes")
print("-" * 70)
resultado_busca = buscar_noticias(resultado_nlp['query_busca'])

# Verificar URLs da busca
for fonte_nome, resultados in resultado_busca.items():
    if fonte_nome == 'metadata':
        continue
    
    if resultados:
        print(f"\n{fonte_nome}:")
        for i, r in enumerate(resultados[:2], 1):
            url = r.get('url', '')
            print(f"  {i}. Tamanho URL: {len(url)} caracteres")
            print(f"     URL: {url}")
            print(f"     Completa? {'âœ…' if url.startswith('http') and len(url) > 50 else 'âŒ'}")

print()
print("=" * 70)
print("ETAPA 3: Scraping")
print("-" * 70)

# Fazer scraping de apenas 1 URL de cada fonte para testar
resultado_busca_reduzido = {}
for fonte_nome, resultados in resultado_busca.items():
    if fonte_nome == 'metadata':
        resultado_busca_reduzido['metadata'] = resultados
    else:
        # Pegar apenas primeira URL
        resultado_busca_reduzido[fonte_nome] = resultados[:1] if resultados else []

resultado_scraping = scrape_noticias(resultado_busca_reduzido)

# Verificar URLs apÃ³s scraping
for fonte_nome, conteudos in resultado_scraping.items():
    if fonte_nome == 'metadata':
        continue
    
    if conteudos:
        print(f"\n{fonte_nome}:")
        for i, c in enumerate(conteudos, 1):
            url = c.get('url', '')
            print(f"  {i}. Tamanho URL: {len(url)} caracteres")
            print(f"     URL: {url}")
            print(f"     Completa? {'âœ…' if len(url) > 50 else 'âŒ'}")
            print(f"     Sucesso scraping? {'âœ…' if c.get('sucesso') else 'âŒ'}")

print()
print("=" * 70)
print("ğŸ¯ ANÃLISE")
print("=" * 70)
print()

# Verificar se alguma URL estÃ¡ truncada
urls_truncadas = []
for fonte_nome, conteudos in resultado_scraping.items():
    if fonte_nome == 'metadata':
        continue
    for c in conteudos:
        url = c.get('url', '')
        if url and len(url) < 50:
            urls_truncadas.append((fonte_nome, url))

if urls_truncadas:
    print("âŒ URLs TRUNCADAS ENCONTRADAS:")
    for fonte, url in urls_truncadas:
        print(f"   â€¢ {fonte}: {url}")
else:
    print("âœ… NENHUMA URL TRUNCADA!")

print()
print("=" * 70)