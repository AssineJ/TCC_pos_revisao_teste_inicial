"""
scraper.py - M√≥dulo de Scraping de Not√≠cias

Responsabilidade:
    Extrair conte√∫do completo das URLs encontradas pelo searcher.
    Usa o mesmo sistema do extractor.py, mas em batch (m√∫ltiplas URLs).

Estrat√©gia:
    - Usar newspaper3k (principal)
    - Fallback para BeautifulSoup
    - Processamento paralelo para velocidade
    - Cache de resultados
    - Tratamento robusto de erros

Autor: Projeto Acad√™mico
Data: 2025
"""

from modules.extractor import ContentExtractor
from concurrent.futures import ThreadPoolExecutor, as_completed
from config import Config
import time
import json
import os
import hashlib
from datetime import datetime


# ============================================================================
# CLASSE DE CACHE PARA SCRAPING
# ============================================================================

class ScraperCache:
    """
    Cache espec√≠fico para resultados de scraping.
    Similar ao SearchCache, mas para conte√∫do extra√≠do.
    """
    
    def __init__(self, cache_dir='cache_scraping'):
        """
        Inicializa cache de scraping.
        
        Args:
            cache_dir (str): Diret√≥rio para cache
        """
        self.cache_dir = cache_dir
        
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir)
    
    
    def _gerar_chave(self, url):
        """
        Gera chave √∫nica para URL.
        
        Args:
            url (str): URL da not√≠cia
            
        Returns:
            str: Hash MD5
        """
        return hashlib.md5(url.encode()).hexdigest()
    
    
    def obter(self, url):
        """
        Obt√©m conte√∫do do cache se existir.
        
        Args:
            url (str): URL da not√≠cia
            
        Returns:
            dict ou None: Conte√∫do ou None
        """
        if not Config.ENABLE_CACHE:
            return None
        
        chave = self._gerar_chave(url)
        cache_file = os.path.join(self.cache_dir, f"{chave}.json")
        
        if not os.path.exists(cache_file):
            return None
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # Verificar expira√ß√£o
            timestamp = cache_data.get('timestamp', 0)
            now = datetime.now().timestamp()
            
            if (now - timestamp) > Config.CACHE_EXPIRATION:
                os.remove(cache_file)
                return None
            
            return cache_data.get('conteudo')
        
        except Exception as e:
            print(f"‚ö†Ô∏è  Erro ao ler cache de scraping: {e}")
            return None
    
    
    def salvar(self, url, conteudo):
        """
        Salva conte√∫do no cache.
        
        Args:
            url (str): URL da not√≠cia
            conteudo (dict): Conte√∫do extra√≠do
        """
        if not Config.ENABLE_CACHE:
            return
        
        chave = self._gerar_chave(url)
        cache_file = os.path.join(self.cache_dir, f"{chave}.json")
        
        cache_data = {
            'url': url,
            'timestamp': datetime.now().timestamp(),
            'conteudo': conteudo
        }
        
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"‚ö†Ô∏è  Erro ao salvar cache de scraping: {e}")


# ============================================================================
# CLASSE PRINCIPAL - NEWS SCRAPER
# ============================================================================

class NewsScraper:
    """
    Scraper especializado em extrair conte√∫do de m√∫ltiplas not√≠cias.
    Otimizado para processar resultados do searcher.
    """
    
    def __init__(self):
        """Inicializa scraper com extractor e cache"""
        self.extractor = ContentExtractor()
        self.cache = ScraperCache()
    
    
    def scrape_resultados_busca(self, resultados_busca):
        """
        Extrai conte√∫do de todos os resultados da busca.
        
        Args:
            resultados_busca (dict): Resultado do searcher.buscar_em_todas_fontes()
            
        Returns:
            dict: Conte√∫dos extra√≠dos ou dados da busca (t√≠tulo+snippet)
        """
        print(f"\nüì• Iniciando scraping de not√≠cias encontradas...")
        
        # Fontes problem√°ticas que devem usar apenas t√≠tulo+snippet
        fontes_problematicas = Config.SOURCES_WITH_PAYWALL
        
        resultados_scraping = {}
        total_urls = 0
        total_sucesso = 0
        
        # Para cada fonte
        for fonte_nome, fonte_resultados in resultados_busca.items():
            if fonte_nome == 'metadata':
                continue
            
            if not fonte_resultados:
                resultados_scraping[fonte_nome] = []
                continue
            
            print(f"\n  üì∞ {fonte_nome}: {len(fonte_resultados)} URL(s)")
            
            # Verificar se √© fonte problem√°tica
            if fonte_nome in fontes_problematicas:
                print(f"    ‚ö†Ô∏è  Fonte com paywall detectada - usando t√≠tulo+snippet")
                conteudos = self._usar_titulo_snippet(fonte_resultados)
                sucessos = len([c for c in conteudos if c['sucesso']])
            else:
                # Extrair conte√∫do normalmente
                urls = [item['url'] for item in fonte_resultados]
                conteudos = self.scrape_urls(urls)
                sucessos = sum(1 for c in conteudos if c['sucesso'])
            
            resultados_scraping[fonte_nome] = conteudos
            
            total_urls += len(fonte_resultados)
            total_sucesso += sucessos
            
            print(f"    ‚úÖ {sucessos}/{len(fonte_resultados)} processados")
        
        # Calcular taxa de sucesso
        taxa_sucesso = (total_sucesso / total_urls * 100) if total_urls > 0 else 0
        
        print(f"\n‚úÖ Scraping conclu√≠do: {total_sucesso}/{total_urls} ({taxa_sucesso:.1f}%)")
        
        return {
            **resultados_scraping,
            'metadata': {
                'total_scraped': total_urls,
                'total_sucesso': total_sucesso,
                'total_falhas': total_urls - total_sucesso,
                'taxa_sucesso': round(taxa_sucesso, 2)
            }
        }
    
    
    def _usar_titulo_snippet(self, resultados_busca):
        """
        Usa t√≠tulo + snippet como "conte√∫do" sem fazer scraping.
        Para fontes com paywall que bloqueiam acesso.
        
        Args:
            resultados_busca (list): Lista de resultados da busca
            
        Returns:
            list: Lista com "conte√∫do" montado a partir de t√≠tulo+snippet
        """
        conteudos = []
        
        for resultado in resultados_busca:
            titulo = resultado.get('title', '')
            snippet = resultado.get('snippet', '')
            url = resultado.get('url', '')
            
            # Montar "texto" a partir de t√≠tulo + snippet
            texto_completo = f"{titulo}. {snippet}"
            
            # Se o texto for muito curto, marcar como falha
            if len(texto_completo.strip()) < 30:
                conteudos.append({
                    'url': url,
                    'titulo': titulo,
                    'texto': '',
                    'data_publicacao': None,
                    'autor': None,
                    'metodo_extracao': 'titulo_snippet',
                    'sucesso': False,
                    'erro': 'T√≠tulo+snippet muito curto'
                })
                continue
            
            conteudos.append({
                'url': url,
                'titulo': titulo,
                'texto': texto_completo,
                'data_publicacao': None,
                'autor': None,
                'metodo_extracao': 'titulo_snippet',
                'sucesso': True,
                'erro': None
            })
        
        return conteudos
    
    
    def scrape_urls(self, urls):
        """
        Extrai conte√∫do de m√∫ltiplas URLs em paralelo.
        
        Args:
            urls (list): Lista de URLs
            
        Returns:
            list: Lista de dicion√°rios com conte√∫do extra√≠do
        """
        if not urls:
            return []
        
        resultados = []
        
        # Verificar cache primeiro
        urls_para_scrape = []
        for url in urls:
            cache_result = self.cache.obter(url)
            if cache_result:
                print(f"      üíæ Cache hit: {url[:50]}...")
                resultados.append(cache_result)
            else:
                urls_para_scrape.append(url)
        
        # Se n√£o h√° URLs para scrape (tudo em cache)
        if not urls_para_scrape:
            return resultados
        
        # Processar URLs em paralelo (m√°ximo 3 threads simult√¢neas)
        max_workers = min(3, len(urls_para_scrape))
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submeter tarefas
            future_to_url = {
                executor.submit(self._scrape_url_single, url): url 
                for url in urls_para_scrape
            }
            
            # Coletar resultados conforme completam
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    resultado = future.result()
                    resultados.append(resultado)
                    
                    # Salvar no cache se sucesso
                    if resultado['sucesso']:
                        self.cache.salvar(url, resultado)
                
                except Exception as e:
                    print(f"      ‚ùå Erro ao processar {url[:50]}: {e}")
                    resultados.append({
                        'url': url,
                        'titulo': None,
                        'texto': None,
                        'data_publicacao': None,
                        'autor': None,
                        'sucesso': False,
                        'erro': str(e)
                    })
        
        return resultados
    
    
    def _scrape_url_single(self, url):
        """
        Extrai conte√∫do de uma √∫nica URL.
        Wrapper para usar com ThreadPoolExecutor.
        
        Args:
            url (str): URL da not√≠cia
            
        Returns:
            dict: Conte√∫do extra√≠do
        """
        # Garantir que URL est√° completa e n√£o truncada
        if not url.startswith('http'):
            url = 'https://' + url
        
        print(f"      üîÑ Extraindo: {url[:80]}...")
        
        # Usar o extractor que j√° criamos
        resultado = self.extractor.extract(url)
        
        # IMPORTANTE: Manter URL original completa no resultado
        resultado['url'] = url  # Garantir que URL n√£o seja truncada
        
        if resultado['sucesso']:
            print(f"      ‚úÖ Sucesso: {resultado['titulo'][:40]}...")
        else:
            print(f"      ‚ùå Falha: {resultado['erro'][:40]}...")
        
        # Pequeno delay para n√£o sobrecarregar
        time.sleep(0.5)
        
        return resultado
    
    
    def scrape_url(self, url):
        """
        M√©todo de conveni√™ncia para extrair uma √∫nica URL.
        
        Args:
            url (str): URL da not√≠cia
            
        Returns:
            dict: Conte√∫do extra√≠do
        """
        # Verificar cache
        cache_result = self.cache.obter(url)
        if cache_result:
            return cache_result
        
        # Extrair
        resultado = self._scrape_url_single(url)
        
        # Salvar cache se sucesso
        if resultado['sucesso']:
            self.cache.salvar(url, resultado)
        
        return resultado


# ============================================================================
# FUN√á√ÉO DE CONVENI√äNCIA
# ============================================================================

def scrape_noticias(resultados_busca):
    """
    Fun√ß√£o simplificada para scrape de resultados de busca.
    
    Args:
        resultados_busca (dict): Resultado do searcher
        
    Returns:
        dict: Conte√∫dos extra√≠dos
        
    Exemplo:
        >>> from modules.searcher import buscar_noticias
        >>> from modules.scraper import scrape_noticias
        >>> 
        >>> # Buscar
        >>> resultados = buscar_noticias("Lula reforma")
        >>> 
        >>> # Extrair conte√∫do
        >>> conteudos = scrape_noticias(resultados)
        >>> 
        >>> # Acessar
        >>> for noticia in conteudos['G1']:
        >>>     if noticia['sucesso']:
        >>>         print(noticia['titulo'])
        >>>         print(noticia['texto'][:200])
    """
    scraper = NewsScraper()
    return scraper.scrape_resultados_busca(resultados_busca)


# ============================================================================
# TESTE DO M√ìDULO
# ============================================================================

if __name__ == "__main__":
    """
    Testes do m√≥dulo scraper.
    Execute: python modules/scraper.py
    """
    
    print("=" * 70)
    print("üß™ TESTANDO M√ìDULO SCRAPER")
    print("=" * 70)
    print()
    
    # Simular resultado de busca (normalmente viria do searcher)
    resultados_busca_mock = {
        'G1': [
            {
                'title': 'Not√≠cia teste 1',
                'url': 'https://g1.globo.com/',  # URL real para testar
                'snippet': 'Snippet de teste'
            }
        ],
        'Folha de S.Paulo': [
            {
                'title': 'Not√≠cia teste 2',
                'url': 'https://www.folha.uol.com.br/',  # URL real para testar
                'snippet': 'Snippet de teste'
            }
        ],
        'metadata': {
            'total_resultados': 2,
            'fontes_com_sucesso': 2
        }
    }
    
    print("üìù Simulando busca com 2 URLs...")
    print()
    
    # Fazer scraping
    conteudos = scrape_noticias(resultados_busca_mock)
    
    # Mostrar resultados
    print("\n" + "=" * 70)
    print("‚úÖ RESULTADOS DO SCRAPING:")
    print("=" * 70)
    print()
    
    for fonte_nome, fonte_conteudos in conteudos.items():
        if fonte_nome == 'metadata':
            continue
        
        print(f"üì∞ {fonte_nome}:")
        for i, conteudo in enumerate(fonte_conteudos, 1):
            print(f"\n  {i}. URL: {conteudo['url'][:60]}...")
            if conteudo['sucesso']:
                print(f"     ‚úÖ T√≠tulo: {conteudo['titulo'][:50]}...")
                print(f"     üìÑ Texto: {len(conteudo['texto'])} caracteres")
                print(f"     üìÖ Data: {conteudo['data_publicacao']}")
                print(f"     ‚úçÔ∏è  Autor: {conteudo['autor']}")
            else:
                print(f"     ‚ùå Erro: {conteudo['erro']}")
        print()
    
    # Metadata
    meta = conteudos['metadata']
    print("üìä ESTAT√çSTICAS:")
    print(f"  Total processado: {meta['total_scraped']}")
    print(f"  Sucessos: {meta['total_sucesso']}")
    print(f"  Falhas: {meta['total_falhas']}")
    print(f"  Taxa de sucesso: {meta['taxa_sucesso']:.1f}%")