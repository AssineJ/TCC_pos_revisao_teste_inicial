"""
scraper.py - MÃ³dulo de Scraping de NotÃ­cias

Responsabilidade:
    Extrair conteÃºdo completo das URLs encontradas pelo searcher.
    Usa o mesmo sistema do extractor.py, mas em batch (mÃºltiplas URLs).

EstratÃ©gia:
    - Usar newspaper3k (principal)
    - Fallback para BeautifulSoup
    - Processamento paralelo para velocidade
    - Cache de resultados
    - Tratamento robusto de erros

Autor: Projeto AcadÃªmico
Data: 2025
"""

from modules.extractor import ContentExtractor
from concurrent.futures import ThreadPoolExecutor, as_completed
from config import Config
import time
import json
import os
import hashlib
from datetime import datetime


# ============================================================================
# CLASSE DE CACHE PARA SCRAPING
# ============================================================================

class ScraperCache:
    """
    Cache especÃ­fico para resultados de scraping.
    Similar ao SearchCache, mas para conteÃºdo extraÃ­do.
    """
    
    def __init__(self, cache_dir='cache_scraping'):
        """
        Inicializa cache de scraping.
        
        Args:
            cache_dir (str): DiretÃ³rio para cache
        """
        self.cache_dir = cache_dir
        
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir)
    
    
    def _gerar_chave(self, url):
        """
        Gera chave Ãºnica para URL.
        
        Args:
            url (str): URL da notÃ­cia
            
        Returns:
            str: Hash MD5
        """
        return hashlib.md5(url.encode()).hexdigest()
    
    
    def obter(self, url):
        """
        ObtÃ©m conteÃºdo do cache se existir.
        
        Args:
            url (str): URL da notÃ­cia
            
        Returns:
            dict ou None: ConteÃºdo ou None
        """
        if not Config.ENABLE_CACHE:
            return None
        
        chave = self._gerar_chave(url)
        cache_file = os.path.join(self.cache_dir, f"{chave}.json")
        
        if not os.path.exists(cache_file):
            return None
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # Verificar expiraÃ§Ã£o
            timestamp = cache_data.get('timestamp', 0)
            now = datetime.now().timestamp()
            
            if (now - timestamp) > Config.CACHE_EXPIRATION:
                os.remove(cache_file)
                return None
            
            return cache_data.get('conteudo')
        
        except Exception as e:
            print(f"âš ï¸  Erro ao ler cache de scraping: {e}")
            return None
    
    
    def salvar(self, url, conteudo):
        """
        Salva conteÃºdo no cache.
        
        Args:
            url (str): URL da notÃ­cia
            conteudo (dict): ConteÃºdo extraÃ­do
        """
        if not Config.ENABLE_CACHE:
            return
        
        chave = self._gerar_chave(url)
        cache_file = os.path.join(self.cache_dir, f"{chave}.json")
        
        cache_data = {
            'url': url,
            'timestamp': datetime.now().timestamp(),
            'conteudo': conteudo
        }
        
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  Erro ao salvar cache de scraping: {e}")


# ============================================================================
# CLASSE PRINCIPAL - NEWS SCRAPER
# ============================================================================

class NewsScraper:
    """
    Scraper especializado em extrair conteÃºdo de mÃºltiplas notÃ­cias.
    Otimizado para processar resultados do searcher.
    """
    
    def __init__(self):
        """Inicializa scraper com extractor e cache"""
        self.extractor = ContentExtractor()
        self.cache = ScraperCache()
    
    
    def scrape_resultados_busca(self, resultados_busca):
        """
        Extrai conteÃºdo de todos os resultados da busca.
        
        Args:
            resultados_busca (dict): Resultado do searcher.buscar_em_todas_fontes()
            {
                'G1': [{'title': ..., 'url': ..., 'snippet': ...}],
                'Folha': [...],
                'metadata': {...}
            }
            
        Returns:
            dict: {
                'G1': [{'url': ..., 'titulo': ..., 'texto': ..., 'sucesso': ...}],
                'Folha': [...],
                'metadata': {
                    'total_scraped': int,
                    'total_sucesso': int,
                    'taxa_sucesso': float
                }
            }
        """
        print(f"\nğŸ“¥ Iniciando scraping de notÃ­cias encontradas...")
        
        resultados_scraping = {}
        total_urls = 0
        total_sucesso = 0
        
        # Para cada fonte
        for fonte_nome, fonte_resultados in resultados_busca.items():
            if fonte_nome == 'metadata':
                continue
            
            if not fonte_resultados:
                resultados_scraping[fonte_nome] = []
                continue
            
            print(f"\n  ğŸ“° {fonte_nome}: {len(fonte_resultados)} URL(s) para extrair")
            
            # Extrair conteÃºdo de cada URL desta fonte
            urls = [item['url'] for item in fonte_resultados]
            conteudos = self.scrape_urls(urls)
            
            resultados_scraping[fonte_nome] = conteudos
            
            # Contar sucessos
            sucessos = sum(1 for c in conteudos if c['sucesso'])
            total_urls += len(urls)
            total_sucesso += sucessos
            
            print(f"    âœ… {sucessos}/{len(urls)} extraÃ§Ãµes bem-sucedidas")
        
        # Calcular taxa de sucesso
        taxa_sucesso = (total_sucesso / total_urls * 100) if total_urls > 0 else 0
        
        print(f"\nâœ… Scraping concluÃ­do: {total_sucesso}/{total_urls} ({taxa_sucesso:.1f}%)")
        
        return {
            **resultados_scraping,
            'metadata': {
                'total_scraped': total_urls,
                'total_sucesso': total_sucesso,
                'total_falhas': total_urls - total_sucesso,
                'taxa_sucesso': round(taxa_sucesso, 2)
            }
        }
    
    
    def scrape_urls(self, urls):
        """
        Extrai conteÃºdo de mÃºltiplas URLs em paralelo.
        
        Args:
            urls (list): Lista de URLs
            
        Returns:
            list: Lista de dicionÃ¡rios com conteÃºdo extraÃ­do
        """
        if not urls:
            return []
        
        resultados = []
        
        # Verificar cache primeiro
        urls_para_scrape = []
        for url in urls:
            cache_result = self.cache.obter(url)
            if cache_result:
                print(f"      ğŸ’¾ Cache hit: {url[:50]}...")
                resultados.append(cache_result)
            else:
                urls_para_scrape.append(url)
        
        # Se nÃ£o hÃ¡ URLs para scrape (tudo em cache)
        if not urls_para_scrape:
            return resultados
        
        # Processar URLs em paralelo (mÃ¡ximo 3 threads simultÃ¢neas)
        max_workers = min(3, len(urls_para_scrape))
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submeter tarefas
            future_to_url = {
                executor.submit(self._scrape_url_single, url): url 
                for url in urls_para_scrape
            }
            
            # Coletar resultados conforme completam
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    resultado = future.result()
                    resultados.append(resultado)
                    
                    # Salvar no cache se sucesso
                    if resultado['sucesso']:
                        self.cache.salvar(url, resultado)
                
                except Exception as e:
                    print(f"      âŒ Erro ao processar {url[:50]}: {e}")
                    resultados.append({
                        'url': url,
                        'titulo': None,
                        'texto': None,
                        'data_publicacao': None,
                        'autor': None,
                        'sucesso': False,
                        'erro': str(e)
                    })
        
        return resultados
    
    
    def _scrape_url_single(self, url):
        """
        Extrai conteÃºdo de uma Ãºnica URL.
        Wrapper para usar com ThreadPoolExecutor.
        
        Args:
            url (str): URL da notÃ­cia
            
        Returns:
            dict: ConteÃºdo extraÃ­do
        """
        # Garantir que URL estÃ¡ completa e nÃ£o truncada
        if not url.startswith('http'):
            url = 'https://' + url
        
        print(f"      ğŸ”„ Extraindo: {url[:80]}...")
        
        # Usar o extractor que jÃ¡ criamos
        resultado = self.extractor.extract(url)
        
        # IMPORTANTE: Manter URL original completa no resultado
        resultado['url'] = url  # Garantir que URL nÃ£o seja truncada
        
        if resultado['sucesso']:
            print(f"      âœ… Sucesso: {resultado['titulo'][:40]}...")
        else:
            print(f"      âŒ Falha: {resultado['erro'][:40]}...")
        
        # Pequeno delay para nÃ£o sobrecarregar
        time.sleep(0.5)
        
        return resultado
    
    
    def scrape_url(self, url):
        """
        MÃ©todo de conveniÃªncia para extrair uma Ãºnica URL.
        
        Args:
            url (str): URL da notÃ­cia
            
        Returns:
            dict: ConteÃºdo extraÃ­do
        """
        # Verificar cache
        cache_result = self.cache.obter(url)
        if cache_result:
            return cache_result
        
        # Extrair
        resultado = self._scrape_url_single(url)
        
        # Salvar cache se sucesso
        if resultado['sucesso']:
            self.cache.salvar(url, resultado)
        
        return resultado


# ============================================================================
# FUNÃ‡ÃƒO DE CONVENIÃŠNCIA
# ============================================================================

def scrape_noticias(resultados_busca):
    """
    FunÃ§Ã£o simplificada para scrape de resultados de busca.
    
    Args:
        resultados_busca (dict): Resultado do searcher
        
    Returns:
        dict: ConteÃºdos extraÃ­dos
        
    Exemplo:
        >>> from modules.searcher import buscar_noticias
        >>> from modules.scraper import scrape_noticias
        >>> 
        >>> # Buscar
        >>> resultados = buscar_noticias("Lula reforma")
        >>> 
        >>> # Extrair conteÃºdo
        >>> conteudos = scrape_noticias(resultados)
        >>> 
        >>> # Acessar
        >>> for noticia in conteudos['G1']:
        >>>     if noticia['sucesso']:
        >>>         print(noticia['titulo'])
        >>>         print(noticia['texto'][:200])
    """
    scraper = NewsScraper()
    return scraper.scrape_resultados_busca(resultados_busca)


# ============================================================================
# TESTE DO MÃ“DULO
# ============================================================================

if __name__ == "__main__":
    """
    Testes do mÃ³dulo scraper.
    Execute: python modules/scraper.py
    """
    
    print("=" * 70)
    print("ğŸ§ª TESTANDO MÃ“DULO SCRAPER")
    print("=" * 70)
    print()
    
    # Simular resultado de busca (normalmente viria do searcher)
    resultados_busca_mock = {
        'G1': [
            {
                'title': 'NotÃ­cia teste 1',
                'url': 'https://g1.globo.com/',  # URL real para testar
                'snippet': 'Snippet de teste'
            }
        ],
        'Folha de S.Paulo': [
            {
                'title': 'NotÃ­cia teste 2',
                'url': 'https://www.folha.uol.com.br/',  # URL real para testar
                'snippet': 'Snippet de teste'
            }
        ],
        'metadata': {
            'total_resultados': 2,
            'fontes_com_sucesso': 2
        }
    }
    
    print("ğŸ“ Simulando busca com 2 URLs...")
    print()
    
    # Fazer scraping
    conteudos = scrape_noticias(resultados_busca_mock)
    
    # Mostrar resultados
    print("\n" + "=" * 70)
    print("âœ… RESULTADOS DO SCRAPING:")
    print("=" * 70)
    print()
    
    for fonte_nome, fonte_conteudos in conteudos.items():
        if fonte_nome == 'metadata':
            continue
        
        print(f"ğŸ“° {fonte_nome}:")
        for i, conteudo in enumerate(fonte_conteudos, 1):
            print(f"\n  {i}. URL: {conteudo['url'][:60]}...")
            if conteudo['sucesso']:
                print(f"     âœ… TÃ­tulo: {conteudo['titulo'][:50]}...")
                print(f"     ğŸ“„ Texto: {len(conteudo['texto'])} caracteres")
                print(f"     ğŸ“… Data: {conteudo['data_publicacao']}")
                print(f"     âœï¸  Autor: {conteudo['autor']}")
            else:
                print(f"     âŒ Erro: {conteudo['erro']}")
        print()
    
    # Metadata
    meta = conteudos['metadata']
    print("ğŸ“Š ESTATÃSTICAS:")
    print(f"  Total processado: {meta['total_scraped']}")
    print(f"  Sucessos: {meta['total_sucesso']}")
    print(f"  Falhas: {meta['total_falhas']}")
    print(f"  Taxa de sucesso: {meta['taxa_sucesso']:.1f}%")