"""
nlp_processor.py - M√≥dulo de Processamento de Linguagem Natural

VERS√ÉO CORRIGIDA - Aceita t√≠tulo opcional para melhorar query

Autor: Projeto Acad√™mico
Data: 2025
"""

import spacy
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import re
from config import Config


# Vari√°veis globais para armazenar modelos carregados
_nlp_model = None
_stopwords_pt = None


def _carregar_spacy():
    """Carrega o modelo spaCy apenas uma vez (lazy loading)."""
    global _nlp_model
    
    if _nlp_model is None:
        print("üìö Carregando modelo spaCy (pt_core_news_lg)...")
        try:
            _nlp_model = spacy.load(Config.SPACY_MODEL)
            print("‚úÖ Modelo spaCy carregado!")
        except OSError:
            print("‚ùå ERRO: Modelo spaCy n√£o encontrado!")
            print("Execute: python -m spacy download pt_core_news_lg")
            raise
    
    return _nlp_model


def _carregar_stopwords():
    """Carrega stopwords do NLTK + customizadas."""
    global _stopwords_pt
    
    if _stopwords_pt is None:
        print("üìö Carregando stopwords...")
        try:
            nltk_stopwords = set(stopwords.words('portuguese'))
            custom_stopwords = set(Config.CUSTOM_STOPWORDS)
            _stopwords_pt = nltk_stopwords.union(custom_stopwords)
            print(f"‚úÖ {len(_stopwords_pt)} stopwords carregadas!")
        except LookupError:
            print("‚ùå ERRO: Stopwords do NLTK n√£o encontradas!")
            print("Execute: python -c \"import nltk; nltk.download('stopwords')\"")
            raise
    
    return _stopwords_pt


class NLPProcessor:
    """Classe respons√°vel por processar texto usando IA."""
    
    def __init__(self):
        """Inicializa processador com modelos de IA"""
        self.nlp = _carregar_spacy()
        self.stopwords = _carregar_stopwords()
    
    
    def processar(self, texto, titulo=None):
        """
        Processa texto completo e extrai todas as informa√ß√µes.
        
        Args:
            texto (str): Texto da not√≠cia
            titulo (str, optional): T√≠tulo da not√≠cia (melhora a query)
            
        Returns:
            dict: Informa√ß√µes extra√≠das
        """
        # Limpar textos
        texto_limpo = self._limpar_texto(texto)
        titulo_limpo = self._limpar_texto(titulo) if titulo else None
        
        # Processar com spaCy
        print("ü§ñ Processando texto com IA (spaCy)...")
        doc = self.nlp(texto_limpo)
        
        # Extrair entidades e palavras-chave do texto
        entidades = self._extrair_entidades(doc)
        palavras_chave = self._extrair_palavras_chave(doc)
        
        # Se tiver t√≠tulo, processar tamb√©m
        if titulo_limpo:
            doc_titulo = self.nlp(titulo_limpo)
            entidades_titulo = self._extrair_entidades(doc_titulo)
            palavras_titulo = self._extrair_palavras_chave(doc_titulo)
            
            # Gerar query priorizando t√≠tulo
            query_busca = self._gerar_query_com_titulo(
                entidades_titulo, palavras_titulo,
                entidades, palavras_chave
            )
        else:
            # Gerar query apenas com texto
            query_busca = self._gerar_query(entidades, palavras_chave)
        
        # Remover stopwords
        texto_sem_stopwords = self._remover_stopwords(texto_limpo)
        
        # Estat√≠sticas
        estatisticas = {
            'total_tokens': len(doc),
            'total_entidades': len(entidades),
            'total_palavras_chave': len(palavras_chave),
            'tamanho_texto_original': len(texto),
            'tamanho_texto_limpo': len(texto_limpo)
        }
        
        return {
            'entidades': entidades,
            'palavras_chave': palavras_chave,
            'query_busca': query_busca,
            'texto_limpo': texto_sem_stopwords,
            'estatisticas': estatisticas
        }
    
    
    def _limpar_texto(self, texto):
        """Limpa texto removendo caracteres especiais."""
        if not texto:
            return ""
        
        texto = re.sub(r'http\S+|www\S+', '', texto)
        texto = re.sub(r'\S+@\S+', '', texto)
        texto = re.sub(r'\s+', ' ', texto)
        texto = texto.strip()
        return texto
    
    
    def _extrair_entidades(self, doc):
        """Extrai entidades nomeadas usando spaCy."""
        entidades = []
        
        for ent in doc.ents:
            if ent.label_ in Config.ENTITY_TYPES:
                entidades.append({
                    'texto': ent.text,
                    'tipo': ent.label_,
                    'importancia': self._calcular_importancia_entidade(ent)
                })
        
        # Remover duplicatas
        entidades_unicas = {}
        for ent in entidades:
            texto_lower = ent['texto'].lower()
            if texto_lower not in entidades_unicas or ent['importancia'] > entidades_unicas[texto_lower]['importancia']:
                entidades_unicas[texto_lower] = ent
        
        entidades = list(entidades_unicas.values())
        entidades.sort(key=lambda x: x['importancia'], reverse=True)
        entidades = entidades[:Config.MAX_ENTITIES]
        
        return entidades
    
    
    def _calcular_importancia_entidade(self, ent):
        """Calcula import√¢ncia de uma entidade."""
        posicao_relativa = ent.start / len(ent.doc) if len(ent.doc) > 0 else 0
        importancia_posicao = 1 - (posicao_relativa * 0.5)
        tamanho = len(ent.text.split())
        importancia_tamanho = min(tamanho / 3, 1.0)
        importancia = (importancia_posicao * 0.7) + (importancia_tamanho * 0.3)
        return round(importancia, 2)
    
    
    def _extrair_palavras_chave(self, doc):
        """Extrai palavras-chave relevantes."""
        palavras_relevantes = []
        
        for token in doc:
            if (not token.is_stop and 
                token.pos_ in ['NOUN', 'VERB', 'PROPN', 'ADJ'] and
                len(token.text) >= 3 and
                token.is_alpha):
                
                palavra = token.lemma_.lower()
                palavras_relevantes.append(palavra)
        
        contador = Counter(palavras_relevantes)
        palavras_chave = [palavra for palavra, freq in contador.most_common(Config.MAX_KEYWORDS)]
        
        return palavras_chave
    
    
    def _gerar_query_com_titulo(self, entidades_titulo, palavras_titulo, entidades_texto, palavras_texto):
        """
        Gera query priorizando informa√ß√µes do t√≠tulo.
        T√≠tulo tem mais peso por ser mais espec√≠fico e relevante.
        """
        # Priorizar t√≠tulo
        top_ent_titulo = [e['texto'] for e in entidades_titulo[:2]]
        top_pal_titulo = palavras_titulo[:3]
        
        # Complementar com texto
        top_ent_texto = [e['texto'] for e in entidades_texto[:1]]
        top_pal_texto = palavras_texto[:2]
        
        # Combinar (t√≠tulo primeiro)
        termos = top_ent_titulo + top_pal_titulo + top_ent_texto + top_pal_texto
        
        # Remover duplicatas mantendo ordem
        termos_unicos = []
        for termo in termos:
            if termo.lower() not in [t.lower() for t in termos_unicos]:
                termos_unicos.append(termo)
        
        # Limitar a 6 termos
        query = ' '.join(termos_unicos[:6])
        return query
    
    
    def _gerar_query(self, entidades, palavras_chave):
        """Gera query apenas com informa√ß√µes do texto."""
        top_entidades = [ent['texto'] for ent in entidades[:3]]
        top_palavras = palavras_chave[:5]
        
        termos_busca = top_entidades + top_palavras
        
        # Remover duplicatas
        termos_unicos = []
        for termo in termos_busca:
            if termo.lower() not in [t.lower() for t in termos_unicos]:
                termos_unicos.append(termo)
        
        termos_unicos = termos_unicos[:6]
        query = ' '.join(termos_unicos)
        return query
    
    
    def _remover_stopwords(self, texto):
        """Remove stopwords do texto."""
        palavras = word_tokenize(texto.lower(), language='portuguese')
        palavras_filtradas = [
            palavra for palavra in palavras 
            if palavra.isalpha() and palavra not in self.stopwords and len(palavra) >= 3
        ]
        texto_filtrado = ' '.join(palavras_filtradas)
        return texto_filtrado


def processar_texto(texto, titulo=None):
    """
    Fun√ß√£o simplificada para processar texto.
    
    Args:
        texto (str): Texto a processar
        titulo (str, optional): T√≠tulo para melhorar query
        
    Returns:
        dict: Resultado do processamento
    """
    processor = NLPProcessor()
    return processor.processar(texto, titulo)


if __name__ == "__main__":
    print("=" * 70)
    print("üß™ TESTANDO M√ìDULO NLP PROCESSOR")
    print("=" * 70)
    print()
    
    texto_teste = """
    O presidente Luiz In√°cio Lula da Silva anunciou ontem em Bras√≠lia uma 
    importante reforma tribut√°ria que reduzir√° os impostos sobre alimentos 
    b√°sicos em todo o Brasil.
    """
    
    titulo_teste = "Lula anuncia reforma tribut√°ria em Bras√≠lia"
    
    print("üìù Texto de teste:")
    print(texto_teste.strip())
    print()
    print(f"üì∞ T√≠tulo: {titulo_teste}")
    print()
    
    # Testar COM t√≠tulo
    print("Teste 1: COM t√≠tulo")
    print("-" * 70)
    resultado = processar_texto(texto_teste, titulo=titulo_teste)
    print(f"Query gerada: {resultado['query_busca']}")
    print()
    
    # Testar SEM t√≠tulo
    print("Teste 2: SEM t√≠tulo")
    print("-" * 70)
    resultado2 = processar_texto(texto_teste)
    print(f"Query gerada: {resultado2['query_busca']}")
    print()
    
    print("‚úÖ Testes conclu√≠dos!")