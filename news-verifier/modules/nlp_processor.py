import spacy
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import re
from config import Config


# ============================================================================
# CARREGAR MODELOS (LAZY LOADING)
# ============================================================================

# Vari√°veis globais para armazenar modelos carregados
_nlp_model = None
_stopwords_pt = None


def _carregar_spacy():
    """
    Carrega o modelo spaCy apenas uma vez (lazy loading).
    Importante: Carregar modelo √© pesado, fazemos apenas 1 vez.
    
    Returns:
        spacy.Language: Modelo carregado
    """
    global _nlp_model
    
    if _nlp_model is None:
        print("üìö Carregando modelo spaCy (pt_core_news_lg)...")
        try:
            _nlp_model = spacy.load(Config.SPACY_MODEL)
            print("‚úÖ Modelo spaCy carregado!")
        except OSError:
            print("‚ùå ERRO: Modelo spaCy n√£o encontrado!")
            print("Execute: python -m spacy download pt_core_news_lg")
            raise
    
    return _nlp_model


def _carregar_stopwords():
    """
    Carrega stopwords do NLTK + customizadas do config.
    
    Returns:
        set: Conjunto de stopwords
    """
    global _stopwords_pt
    
    if _stopwords_pt is None:
        print("üìö Carregando stopwords...")
        try:
            # Stopwords do NLTK
            nltk_stopwords = set(stopwords.words('portuguese'))
            
            # Adicionar stopwords customizadas do config
            custom_stopwords = set(Config.CUSTOM_STOPWORDS)
            
            # Combinar
            _stopwords_pt = nltk_stopwords.union(custom_stopwords)
            print(f"‚úÖ {len(_stopwords_pt)} stopwords carregadas!")
        
        except LookupError:
            print("‚ùå ERRO: Stopwords do NLTK n√£o encontradas!")
            print("Execute: python -c \"import nltk; nltk.download('stopwords')\"")
            raise
    
    return _stopwords_pt


# ============================================================================
# CLASSE PRINCIPAL - NLP PROCESSOR
# ============================================================================

class NLPProcessor:
    """
    Classe respons√°vel por processar texto usando IA.
    
    Atributos:
        nlp: Modelo spaCy carregado
        stopwords: Conjunto de stopwords
    """
    
    def __init__(self):
        """Inicializa processador com modelos de IA"""
        self.nlp = _carregar_spacy()
        self.stopwords = _carregar_stopwords()
    
    
    def processar(self, texto):
        """
        Processa texto completo e extrai todas as informa√ß√µes.
        
        Args:
            texto (str): Texto da not√≠cia
            
        Returns:
            dict: {
                'entidades': list,           # Entidades nomeadas encontradas
                'palavras_chave': list,      # Palavras-chave mais relevantes
                'query_busca': str,          # Query otimizada para busca
                'texto_limpo': str,          # Texto sem stopwords
                'estatisticas': dict         # Estat√≠sticas do processamento
            }
        """
        
        # ====================================================================
        # ETAPA 1: LIMPAR E NORMALIZAR TEXTO
        # ====================================================================
        
        texto_limpo = self._limpar_texto(texto)
        
        
        # ====================================================================
        # ETAPA 2: PROCESSAR COM SPACY (IA!)
        # ====================================================================
        
        print("ü§ñ Processando texto com IA (spaCy)...")
        doc = self.nlp(texto_limpo)
        
        
        # ====================================================================
        # ETAPA 3: EXTRAIR ENTIDADES NOMEADAS
        # ====================================================================
        
        entidades = self._extrair_entidades(doc)
        
        
        # ====================================================================
        # ETAPA 4: EXTRAIR PALAVRAS-CHAVE
        # ====================================================================
        
        palavras_chave = self._extrair_palavras_chave(doc)
        
        
        # ====================================================================
        # ETAPA 5: GERAR QUERY DE BUSCA
        # ====================================================================
        
        query_busca = self._gerar_query_busca(entidades, palavras_chave)
        
        
        # ====================================================================
        # ETAPA 6: REMOVER STOPWORDS DO TEXTO
        # ====================================================================
        
        texto_sem_stopwords = self._remover_stopwords(texto_limpo)
        
        
        # ====================================================================
        # ETAPA 7: COLETAR ESTAT√çSTICAS
        # ====================================================================
        
        estatisticas = {
            'total_tokens': len(doc),
            'total_entidades': len(entidades),
            'total_palavras_chave': len(palavras_chave),
            'tamanho_texto_original': len(texto),
            'tamanho_texto_limpo': len(texto_limpo)
        }
        
        
        # ====================================================================
        # RETORNAR RESULTADO
        # ====================================================================
        
        return {
            'entidades': entidades,
            'palavras_chave': palavras_chave,
            'query_busca': query_busca,
            'texto_limpo': texto_sem_stopwords,
            'estatisticas': estatisticas
        }
    
    
    def _limpar_texto(self, texto):
        """
        Limpa texto removendo caracteres especiais, m√∫ltiplos espa√ßos, etc.
        
        Args:
            texto (str): Texto original
            
        Returns:
            str: Texto limpo
        """
        # Remover URLs
        texto = re.sub(r'http\S+|www\S+', '', texto)
        
        # Remover emails
        texto = re.sub(r'\S+@\S+', '', texto)
        
        # Remover m√∫ltiplos espa√ßos
        texto = re.sub(r'\s+', ' ', texto)
        
        # Remover espa√ßos no in√≠cio e fim
        texto = texto.strip()
        
        return texto
    
    
    def _extrair_entidades(self, doc):
        """
        Extrai entidades nomeadas do texto usando spaCy.
        
        Entidades s√£o: pessoas, lugares, organiza√ß√µes, datas, etc.
        Essas s√£o as informa√ß√µes mais importantes para buscar nas fontes.
        
        Args:
            doc (spacy.tokens.Doc): Documento processado pelo spaCy
            
        Returns:
            list: Lista de dicion√°rios com entidades
            [
                {'texto': 'Lula', 'tipo': 'PERSON'},
                {'texto': 'Brasil', 'tipo': 'LOC'},
                ...
            ]
        """
        entidades = []
        
        for ent in doc.ents:
            # Filtrar apenas tipos de entidades relevantes
            if ent.label_ in Config.ENTITY_TYPES:
                entidades.append({
                    'texto': ent.text,
                    'tipo': ent.label_,
                    'importancia': self._calcular_importancia_entidade(ent)
                })
        
        # Remover duplicatas (manter a com maior import√¢ncia)
        entidades_unicas = {}
        for ent in entidades:
            texto_lower = ent['texto'].lower()
            if texto_lower not in entidades_unicas or ent['importancia'] > entidades_unicas[texto_lower]['importancia']:
                entidades_unicas[texto_lower] = ent
        
        # Converter de volta para lista e ordenar por import√¢ncia
        entidades = list(entidades_unicas.values())
        entidades.sort(key=lambda x: x['importancia'], reverse=True)
        
        # Limitar ao m√°ximo configurado
        entidades = entidades[:Config.MAX_ENTITIES]
        
        return entidades
    
    
    def _calcular_importancia_entidade(self, ent):
        """
        Calcula import√¢ncia de uma entidade (0-1).
        Entidades no in√≠cio do texto t√™m mais import√¢ncia.
        
        Args:
            ent (spacy.tokens.Span): Entidade do spaCy
            
        Returns:
            float: Import√¢ncia entre 0 e 1
        """
        # Posi√ß√£o no documento (quanto mais no in√≠cio, maior a import√¢ncia)
        posicao_relativa = ent.start / len(ent.doc)
        importancia_posicao = 1 - (posicao_relativa * 0.5)  # M√°ximo 50% de redu√ß√£o
        
        # Tamanho da entidade (entidades maiores tendem a ser mais espec√≠ficas)
        tamanho = len(ent.text.split())
        importancia_tamanho = min(tamanho / 3, 1.0)  # Max 1.0
        
        # M√©dia ponderada
        importancia = (importancia_posicao * 0.7) + (importancia_tamanho * 0.3)
        
        return round(importancia, 2)
    
    
    def _extrair_palavras_chave(self, doc):
        """
        Extrai palavras-chave mais relevantes do texto.
        
        Usa estrat√©gia de frequ√™ncia + import√¢ncia gramatical:
        - Substantivos e verbos s√£o mais importantes
        - Palavras mais frequentes s√£o mais relevantes
        - Remove stopwords
        
        Args:
            doc (spacy.tokens.Doc): Documento processado
            
        Returns:
            list: Lista de palavras-chave ordenadas por relev√¢ncia
        """
        # Coletar palavras relevantes (substantivos, verbos, adjetivos)
        palavras_relevantes = []
        
        for token in doc:
            # Filtros:
            # - N√£o √© stopword
            # - √â substantivo, verbo ou adjetivo
            # - Tem pelo menos 3 caracteres
            # - √â alfab√©tico (n√£o n√∫mero ou pontua√ß√£o)
            if (not token.is_stop and 
                token.pos_ in ['NOUN', 'VERB', 'PROPN', 'ADJ'] and
                len(token.text) >= 3 and
                token.is_alpha):
                
                # Usar lema (forma base da palavra)
                # Exemplo: "correr", "correndo", "correu" -> "correr"
                palavra = token.lemma_.lower()
                palavras_relevantes.append(palavra)
        
        # Contar frequ√™ncias
        contador = Counter(palavras_relevantes)
        
        # Pegar as mais frequentes
        palavras_chave = [palavra for palavra, freq in contador.most_common(Config.MAX_KEYWORDS)]
        
        return palavras_chave
    
    
    def _gerar_query_busca(self, entidades, palavras_chave):
        """
        Gera query otimizada para busca nas fontes.
        
        Combina entidades + palavras-chave de forma inteligente.
        
        Args:
            entidades (list): Lista de entidades extra√≠das
            palavras_chave (list): Lista de palavras-chave
            
        Returns:
            str: Query de busca otimizada
        """
        # Pegar top 3 entidades mais importantes
        top_entidades = [ent['texto'] for ent in entidades[:3]]
        
        # Pegar top 5 palavras-chave
        top_palavras = palavras_chave[:5]
        
        # Combinar (priorizar entidades)
        termos_busca = top_entidades + top_palavras
        
        # Remover duplicatas mantendo ordem
        termos_unicos = []
        for termo in termos_busca:
            if termo.lower() not in [t.lower() for t in termos_unicos]:
                termos_unicos.append(termo)
        
        # Limitar a 6 termos no m√°ximo (queries muito longas n√£o funcionam bem)
        termos_unicos = termos_unicos[:6]
        
        # Juntar com espa√ßos
        query = ' '.join(termos_unicos)
        
        return query
    
    
    def _remover_stopwords(self, texto):
        """
        Remove stopwords do texto mantendo apenas palavras relevantes.
        
        Args:
            texto (str): Texto original
            
        Returns:
            str: Texto sem stopwords
        """
        # Tokenizar
        palavras = word_tokenize(texto.lower(), language='portuguese')
        
        # Filtrar stopwords
        palavras_filtradas = [
            palavra for palavra in palavras 
            if palavra.isalpha() and palavra not in self.stopwords and len(palavra) >= 3
        ]
        
        # Juntar de volta
        texto_filtrado = ' '.join(palavras_filtradas)
        
        return texto_filtrado


# ============================================================================
# FUN√á√ÉO DE CONVENI√äNCIA
# ============================================================================

def processar_texto(texto):
    """
    Fun√ß√£o simplificada para processar texto.
    Esta √© a fun√ß√£o que ser√° chamada por outros m√≥dulos.
    
    Args:
        texto (str): Texto a processar
        
    Returns:
        dict: Resultado do processamento
        
    Exemplo:
        >>> resultado = processar_texto("O presidente anunciou reforma...")
        >>> print(resultado['entidades'])
        >>> print(resultado['query_busca'])
    """
    processor = NLPProcessor()
    return processor.processar(texto)


# ============================================================================
# TESTE DO M√ìDULO
# ============================================================================

if __name__ == "__main__":
    """
    Testes do m√≥dulo nlp_processor.
    Execute: python modules/nlp_processor.py
    """
    
    print("=" * 70)
    print("üß™ TESTANDO M√ìDULO NLP PROCESSOR")
    print("=" * 70)
    print()
    
    # Texto de teste
    texto_teste = """
    O presidente Luiz In√°cio Lula da Silva anunciou ontem em Bras√≠lia uma 
    importante reforma tribut√°ria que reduzir√° os impostos sobre alimentos 
    b√°sicos em todo o Brasil. A medida, que ser√° implementada pelo Minist√©rio 
    da Fazenda a partir de janeiro de 2025, deve beneficiar milh√µes de 
    brasileiros de baixa renda. Segundo especialistas econ√¥micos, a reforma 
    pode reduzir a infla√ß√£o e estimular o consumo das fam√≠lias.
    """
    
    print("üìù Texto de teste:")
    print(texto_teste.strip())
    print()
    print("-" * 70)
    print()
    
    # Processar
    resultado = processar_texto(texto_teste)
    
    # Mostrar resultados
    print("‚úÖ RESULTADO DO PROCESSAMENTO:")
    print()
    
    print("üè∑Ô∏è  ENTIDADES ENCONTRADAS:")
    for ent in resultado['entidades']:
        print(f"  ‚Ä¢ {ent['texto']} ({ent['tipo']}) - Import√¢ncia: {ent['importancia']}")
    print()
    
    print("üîë PALAVRAS-CHAVE:")
    print(f"  {', '.join(resultado['palavras_chave'])}")
    print()
    
    print("üîç QUERY DE BUSCA OTIMIZADA:")
    print(f"  {resultado['query_busca']}")
    print()
    
    print("üìä ESTAT√çSTICAS:")
    for chave, valor in resultado['estatisticas'].items():
        print(f"  ‚Ä¢ {chave}: {valor}")
    print()
    
    print("üìÑ TEXTO LIMPO (primeiros 200 caracteres):")
    print(f"  {resultado['texto_limpo'][:200]}...")