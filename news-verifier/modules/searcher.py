"""
searcher.py - M√≥dulo de Busca em Fontes Confi√°veis

Responsabilidade:
    Buscar not√≠cias relacionadas nas 5 fontes confi√°veis usando:
    1. Mock (dados simulados para desenvolvimento)
    2. SerpAPI (API oficial do Google - recomendado)
    3. googlesearch-python (scraping do Google - fallback)
    4. Busca direta nos sites (scraping - √∫ltimo recurso)

Estrat√©gia H√≠brida:
    - Tenta m√∫ltiplos m√©todos automaticamente
    - Sistema de cache para economizar requisi√ß√µes
    - Fallback autom√°tico se um m√©todo falhar

Autor: Projeto Acad√™mico
Data: 2025
"""

import requests
import time
import json
import os
import hashlib
from datetime import datetime
from bs4 import BeautifulSoup
from config import Config
import random

# Importa√ß√µes condicionais (podem n√£o estar instaladas)
try:
    from serpapi import GoogleSearch
    SERPAPI_AVAILABLE = True
except ImportError:
    SERPAPI_AVAILABLE = False
    print("‚ö†Ô∏è  SerpAPI n√£o dispon√≠vel. Instale com: pip install google-search-results")

try:
    from googlesearch import search as google_search
    GOOGLESEARCH_AVAILABLE = True
except ImportError:
    GOOGLESEARCH_AVAILABLE = False
    print("‚ö†Ô∏è  googlesearch-python n√£o dispon√≠vel. Instale com: pip install googlesearch-python")


# ============================================================================
# CLASSE DE CACHE
# ============================================================================

class SearchCache:
    """
    Gerencia cache de resultados de busca para economizar requisi√ß√µes.
    """
    
    def __init__(self, cache_dir='cache_searches'):
        """
        Inicializa cache.
        
        Args:
            cache_dir (str): Diret√≥rio para armazenar cache
        """
        self.cache_dir = cache_dir
        
        # Criar diret√≥rio se n√£o existir
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir)
    
    
    def _gerar_chave(self, query, site):
        """
        Gera chave √∫nica para query + site.
        
        Args:
            query (str): Query de busca
            site (str): Dom√≠nio do site
            
        Returns:
            str: Hash MD5 como chave
        """
        texto = f"{query}_{site}".lower()
        return hashlib.md5(texto.encode()).hexdigest()
    
    
    def obter(self, query, site):
        """
        Obt√©m resultado do cache se existir e n√£o expirou.
        
        Args:
            query (str): Query de busca
            site (str): Dom√≠nio do site
            
        Returns:
            dict ou None: Resultado em cache ou None
        """
        if not Config.ENABLE_CACHE:
            return None
        
        chave = self._gerar_chave(query, site)
        cache_file = os.path.join(self.cache_dir, f"{chave}.json")
        
        # Verificar se arquivo existe
        if not os.path.exists(cache_file):
            return None
        
        # Ler cache
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # Verificar se expirou
            timestamp = cache_data.get('timestamp', 0)
            now = datetime.now().timestamp()
            
            if (now - timestamp) > Config.CACHE_EXPIRATION:
                # Cache expirado, deletar
                os.remove(cache_file)
                return None
            
            return cache_data.get('resultados')
        
        except Exception as e:
            print(f"‚ö†Ô∏è  Erro ao ler cache: {e}")
            return None
    
    
    def salvar(self, query, site, resultados):
        """
        Salva resultado no cache.
        
        Args:
            query (str): Query de busca
            site (str): Dom√≠nio do site
            resultados (list): Lista de resultados
        """
        if not Config.ENABLE_CACHE:
            return
        
        chave = self._gerar_chave(query, site)
        cache_file = os.path.join(self.cache_dir, f"{chave}.json")
        
        cache_data = {
            'query': query,
            'site': site,
            'timestamp': datetime.now().timestamp(),
            'resultados': resultados
        }
        
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"‚ö†Ô∏è  Erro ao salvar cache: {e}")


# ============================================================================
# CLASSE PRINCIPAL - SEARCH ENGINE
# ============================================================================

class SearchEngine:
    """
    Motor de busca h√≠brido que tenta m√∫ltiplos m√©todos automaticamente.
    """
    
    def __init__(self):
        """Inicializa motor de busca com cache"""
        self.cache = SearchCache()
        self.serpapi_count = 0  # Contador de requisi√ß√µes SerpAPI
    
    
    def buscar_em_todas_fontes(self, query):
        """
        Busca query em todas as fontes confi√°veis.
        
        Args:
            query (str): Query de busca gerada pelo NLP
            
        Returns:
            dict: {
                'fonte1': [resultados],
                'fonte2': [resultados],
                ...
                'metadata': {
                    'total_resultados': int,
                    'fontes_com_sucesso': int,
                    'metodo_usado': str
                }
            }
        """
        print(f"\nüîç Iniciando busca nas {len(Config.TRUSTED_SOURCES)} fontes...")
        print(f"Query: {query}")
        print(f"Modo: {Config.SEARCH_MODE}")
        print()
        
        resultados_por_fonte = {}
        fontes_sucesso = 0
        total_resultados = 0
        
        for fonte in Config.TRUSTED_SOURCES:
            if not fonte['ativo']:
                continue
            
            nome_fonte = fonte['nome']
            dominio = fonte['dominio']
            
            print(f"  Buscando em: {nome_fonte}...")
            
            # Buscar nesta fonte
            resultados = self.buscar(query, dominio)
            
            if resultados:
                resultados_por_fonte[nome_fonte] = resultados
                fontes_sucesso += 1
                total_resultados += len(resultados)
                print(f"    ‚úÖ {len(resultados)} resultados encontrados")
            else:
                resultados_por_fonte[nome_fonte] = []
                print(f"    ‚ùå Nenhum resultado")
            
            # Delay entre buscas
            time.sleep(Config.SEARCH_DELAY)
        
        print(f"\n‚úÖ Busca conclu√≠da: {total_resultados} resultados em {fontes_sucesso} fontes")
        
        return {
            **resultados_por_fonte,
            'metadata': {
                'total_resultados': total_resultados,
                'fontes_com_sucesso': fontes_sucesso,
                'total_fontes': len([f for f in Config.TRUSTED_SOURCES if f['ativo']]),
                'query_original': query,
                'modo_busca': Config.SEARCH_MODE
            }
        }
    
    
    def buscar(self, query, site):
        """
        Busca em um site espec√≠fico usando estrat√©gia h√≠brida.
        
        Args:
            query (str): Query de busca
            site (str): Dom√≠nio do site
            
        Returns:
            list: Lista de resultados [{title, url, snippet}, ...]
        """
        # Verificar cache primeiro
        cache_result = self.cache.obter(query, site)
        if cache_result:
            print(f"    üíæ Cache hit!")
            return cache_result
        
        # Determinar m√©todo(s) a usar
        if Config.SEARCH_MODE == 'mock':
            resultados = self._buscar_mock(query, site)
        
        elif Config.SEARCH_MODE == 'serpapi':
            resultados = self._buscar_serpapi(query, site)
        
        elif Config.SEARCH_MODE == 'googlesearch':
            resultados = self._buscar_googlesearch(query, site)
        
        elif Config.SEARCH_MODE == 'direct':
            resultados = self._buscar_direto(query, site)
        
        elif Config.SEARCH_MODE == 'hybrid':
            # Tentar m√©todos na ordem de prioridade
            resultados = self._buscar_hybrid(query, site)
        
        else:
            print(f"    ‚ö†Ô∏è  Modo desconhecido: {Config.SEARCH_MODE}")
            resultados = []
        
        # Salvar no cache
        if resultados:
            self.cache.salvar(query, site, resultados)
        
        return resultados
    
    
    def _buscar_hybrid(self, query, site):
        """
        Tenta m√∫ltiplos m√©todos at√© obter sucesso.
        
        Args:
            query (str): Query de busca
            site (str): Dom√≠nio do site
            
        Returns:
            list: Resultados
        """
        for metodo in Config.SEARCH_METHODS_PRIORITY:
            try:
                print(f"      Tentando m√©todo: {metodo}")
                
                if metodo == 'serpapi':
                    resultados = self._buscar_serpapi(query, site)
                elif metodo == 'googlesearch':
                    resultados = self._buscar_googlesearch(query, site)
                elif metodo == 'direct':
                    resultados = self._buscar_direto(query, site)
                else:
                    continue
                
                if resultados:
                    print(f"      ‚úÖ Sucesso com {metodo}")
                    return resultados
                
            except Exception as e:
                print(f"      ‚ö†Ô∏è  {metodo} falhou: {str(e)[:50]}...")
                continue
        
        print(f"      ‚ùå Todos os m√©todos falharam")
        return []
    
    
    def _buscar_mock(self, query, site):
        """
        Retorna dados simulados para desenvolvimento.
        N√ÉO faz requisi√ß√µes reais.
        
        Args:
            query (str): Query de busca
            site (str): Dom√≠nio do site
            
        Returns:
            list: Resultados simulados
        """
        # Simular diferentes cen√°rios
        num_resultados = random.randint(1, 3)
        
        resultados = []
        for i in range(num_resultados):
            resultados.append({
                'title': f'Not√≠cia {i+1} sobre {query[:30]} - {site}',
                'url': f'https://{site}/noticia-simulada-{i+1}',
                'snippet': f'Este √© um resultado simulado para testes. Query: {query}. '
                          f'Em produ√ß√£o, aqui aparecer√° o snippet real da not√≠cia encontrada.'
            })
        
        return resultados
    
    
    def _buscar_serpapi(self, query, site):
        """
        Busca usando SerpAPI (Google Search API oficial).
        Requer chave de API.
        
        Args:
            query (str): Query de busca
            site (str): Dom√≠nio do site
            
        Returns:
            list: Resultados
        """
        if not SERPAPI_AVAILABLE:
            raise Exception("SerpAPI n√£o instalado")
        
        if not Config.SERPAPI_KEY:
            raise Exception("SERPAPI_KEY n√£o configurada")
        
        # Verificar limite de requisi√ß√µes
        if self.serpapi_count >= Config.SERPAPI_REQUESTS_LIMIT:
            raise Exception(f"Limite de {Config.SERPAPI_REQUESTS_LIMIT} requisi√ß√µes atingido")
        
        # Construir query com operador site:
        search_query = f"site:{site} {query}"
        
        # Par√¢metros da busca
        params = {
            "q": search_query,
            "api_key": Config.SERPAPI_KEY,
            "num": Config.MAX_SEARCH_RESULTS,
            "hl": "pt-br",
            "gl": "br"
        }
        
        # Fazer requisi√ß√£o
        search = GoogleSearch(params)
        results = search.get_dict()
        
        # Incrementar contador
        self.serpapi_count += 1
        
        # Extrair resultados org√¢nicos
        organic_results = results.get('organic_results', [])
        
        resultados = []
        for item in organic_results[:Config.MAX_SEARCH_RESULTS]:
            # GARANTIR que URL est√° completa
            url = item.get('link', '')
            
            # Valida√ß√£o: URL deve ser completa
            if not url or len(url) < 20:
                continue
            
            # Garantir que come√ßa com http
            if not url.startswith('http'):
                url = 'https://' + url
            
            resultados.append({
                'title': item.get('title', ''),
                'url': url,  # URL COMPLETA E VALIDADA
                'snippet': item.get('snippet', '')
            })
        
        return resultados
    
    
    def _buscar_googlesearch(self, query, site):
        """
        Busca usando googlesearch-python (scraping do Google).
        Gratuito mas pode ser bloqueado.
        
        Args:
            query (str): Query de busca
            site (str): Dom√≠nio do site
            
        Returns:
            list: Resultados
        """
        if not GOOGLESEARCH_AVAILABLE:
            raise Exception("googlesearch-python n√£o instalado")
        
        # Construir query
        search_query = f"site:{site} {query}"
        
        # Fazer busca
        try:
            urls = list(google_search(
                search_query,
                num_results=Config.MAX_SEARCH_RESULTS,
                lang='pt',
                sleep_interval=2  # Delay para n√£o ser bloqueado
            ))
            
            # googlesearch retorna apenas URLs, precisamos buscar t√≠tulo/snippet
            resultados = []
            for url in urls:
                # Tentar extrair t√≠tulo fazendo requisi√ß√£o √† p√°gina
                try:
                    response = requests.get(
                        url,
                        headers=Config.DEFAULT_HEADERS,
                        timeout=5
                    )
                    soup = BeautifulSoup(response.content, 'lxml')
                    
                    # Extrair t√≠tulo
                    title = ''
                    title_tag = soup.find('title')
                    if title_tag:
                        title = title_tag.get_text()
                    
                    # Extrair snippet (primeiro par√°grafo)
                    snippet = ''
                    p_tags = soup.find_all('p', limit=3)
                    if p_tags:
                        snippet = ' '.join([p.get_text()[:100] for p in p_tags])
                    
                    resultados.append({
                        'title': title,
                        'url': url,
                        'snippet': snippet[:200]
                    })
                
                except Exception as e:
                    # Se falhar ao extrair detalhes, adicionar apenas URL
                    resultados.append({
                        'title': url.split('/')[-1],
                        'url': url,
                        'snippet': ''
                    })
            
            return resultados
        
        except Exception as e:
            raise Exception(f"googlesearch falhou: {e}")
    
    
    def _buscar_direto(self, query, site):
        """
        Busca diretamente no site usando p√°gina de busca interna.
        Faz scraping da p√°gina de resultados.
        
        Args:
            query (str): Query de busca
            site (str): Dom√≠nio do site
            
        Returns:
            list: Resultados
        """
        # Encontrar configura√ß√£o da fonte
        fonte = None
        for f in Config.TRUSTED_SOURCES:
            if f['dominio'] == site:
                fonte = f
                break
        
        if not fonte or not fonte.get('url_busca'):
            raise Exception(f"URL de busca n√£o configurada para {site}")
        
        # Construir URL de busca
        search_url = fonte['url_busca'] + query.replace(' ', '+')
        
        # Fazer requisi√ß√£o
        headers = Config.DEFAULT_HEADERS.copy()
        headers['User-Agent'] = random.choice(Config.USER_AGENTS)
        
        response = requests.get(
            search_url,
            headers=headers,
            timeout=Config.SCRAPING_TIMEOUT
        )
        
        response.raise_for_status()
        
        # Parsear HTML
        soup = BeautifulSoup(response.content, 'lxml')
        
        # Estrat√©gia gen√©rica de extra√ß√£o (funciona na maioria dos sites)
        resultados = []
        
        # Procurar por links que parecem ser not√≠cias
        # Cada site tem estrutura diferente, aqui fazemos tentativa gen√©rica
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            
            # Filtros b√°sicos
            if not href or href.startswith('#'):
                continue
            
            # Construir URL completa se relativa
            if href.startswith('/'):
                href = f"https://{site}{href}"
            
            # Verificar se URL pertence ao site
            if site not in href:
                continue
            
            # Extrair t√≠tulo (texto do link ou title attribute)
            title = link.get_text().strip() or link.get('title', '')
            
            if not title or len(title) < 10:
                continue
            
            # Tentar encontrar snippet pr√≥ximo
            snippet = ''
            parent = link.find_parent(['div', 'article', 'li'])
            if parent:
                snippet = parent.get_text()[:200]
            
            resultados.append({
                'title': title[:150],
                'url': href,
                'snippet': snippet
            })
            
            # Limitar resultados
            if len(resultados) >= Config.MAX_SEARCH_RESULTS:
                break
        
        return resultados


# ============================================================================
# FUN√á√ÉO DE CONVENI√äNCIA
# ============================================================================

def buscar_noticias(query):
    """
    Fun√ß√£o simplificada para buscar not√≠cias em todas as fontes.
    
    Args:
        query (str): Query de busca
        
    Returns:
        dict: Resultados por fonte
        
    Exemplo:
        >>> resultados = buscar_noticias("Lula reforma tribut√°ria")
        >>> print(resultados['G1'])
        >>> print(resultados['metadata'])
    """
    engine = SearchEngine()
    return engine.buscar_em_todas_fontes(query)


# ============================================================================
# TESTE DO M√ìDULO
# ============================================================================

if __name__ == "__main__":
    """
    Testes do m√≥dulo searcher.
    Execute: python modules/searcher.py
    """
    
    print("=" * 70)
    print("üß™ TESTANDO M√ìDULO SEARCHER")
    print("=" * 70)
    print()
    
    # Query de teste
    query_teste = "Lula reforma tribut√°ria Brasil"
    
    print(f"üìù Query de teste: {query_teste}")
    print(f"üîß Modo configurado: {Config.SEARCH_MODE}")
    print()
    
    # Buscar
    resultados = buscar_noticias(query_teste)
    
    # Mostrar resultados
    print("\n" + "=" * 70)
    print("‚úÖ RESULTADOS:")
    print("=" * 70)
    print()
    
    for fonte_nome, fonte_resultados in resultados.items():
        if fonte_nome == 'metadata':
            continue
        
        print(f"üì∞ {fonte_nome}:")
        if fonte_resultados:
            for i, item in enumerate(fonte_resultados, 1):
                print(f"  {i}. {item['title'][:60]}...")
                print(f"     URL: {item['url']}")
                print(f"     Snippet: {item['snippet'][:80]}...")
                print()
        else:
            print("  (sem resultados)")
            print()
    
    # Metadata
    meta = resultados['metadata']
    print("üìä ESTAT√çSTICAS:")
    print(f"  Total de resultados: {meta['total_resultados']}")
    print(f"  Fontes com sucesso: {meta['fontes_com_sucesso']}/{meta['total_fontes']}")
    print(f"  Modo usado: {meta['modo_busca']}")