# modules/semantic_analyzer.py
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import re
from config import Config
import time

# === NOVO: cache de embeddings ===
import os
import hashlib
import pickle

# Diret√≥rio de cache (persistente)
CACHE_DIR = os.path.join("cache", "embeddings")
os.makedirs(CACHE_DIR, exist_ok=True)


# ============================================================================
# DETEC√á√ÉO INTELIGENTE DE CONTRADI√á√ÉO
# ============================================================================

# Palavras que indicam NEGA√á√ÉO FORTE (s√≥ estas contam)
STRONG_NEGATION_WORDS = [
    'n√£o', 'nao', 'nunca', 'jamais', 'nenhum', 'nenhuma',
    'falso', 'falsa', 'mentira', 'fake', 'desmentido', 'desmente',
    'incorreto', 'incorreta', 'errado', 'errada',
    'boato', 'desinforma√ß√£o', 'fake news'
]

# Palavras NEUTRAS que N√ÉO s√£o contradi√ß√£o (sin√¥nimos, varia√ß√µes)
NEUTRAL_WORDS = [
    'quer', 'pretende', 'planeja', 'projeta', 'estuda', 'avalia',
    'confirma', 'anuncia', 'declara', 'afirma', 'diz', 'informa',
    'pode', 'deve', 'vai', 'ir√°', 'poder√°', 'dever√°'
]

# Padr√µes FORTES de desmentido (s√≥ estes ativam alerta)
STRONG_DEBUNK_PATTERNS = [
    r'√©\s+falso\s+que',
    r'n√£o\s+√©\s+verdade\s+que',
    r'checagem.*conclu[i√≠].*falso',
    r'fact.*check.*falso',
    r'desmente.*completamente',
    r'totalmente\s+falso',
    r'n√£o\s+h√°\s+nenhuma\s+evid√™ncia',
    r'completamente\s+sem\s+fundamento',
    r'boato\s+que\s+circula'
]


def extrair_numeros(texto):
    """
    Extrai n√∫meros do texto para compara√ß√£o.
    Exemplo: "R$ 1.631" ‚Üí 1631.0
    """
    # Remove formata√ß√£o e extrai n√∫meros
    numeros = re.findall(r'[\d]+[.,]?[\d]*', texto.replace('.', '').replace(',', '.'))
    return [float(n.replace(',', '.')) for n in numeros if n]


def numeros_similares(nums1, nums2, tolerancia=0.05):
    """
    Verifica se n√∫meros s√£o similares (toler√¢ncia de 5% por padr√£o).
    Exemplo: 1631 e 1630 s√£o similares (diferen√ßa < 5%)
    """
    if not nums1 or not nums2:
        return False
    
    for n1 in nums1:
        for n2 in nums2:
            if n1 > 0 and n2 > 0:
                diferenca = abs(n1 - n2) / max(n1, n2)
                if diferenca <= tolerancia:
                    return True
    return False


def detectar_contradicao_inteligente(texto_original, texto_fonte, similaridade_semantica):
    """
    Detecta contradi√ß√£o de forma INTELIGENTE, evitando falsos positivos.
    """
    texto_fonte_lower = texto_fonte.lower()
    texto_original_lower = texto_original.lower()
    
    evidencias = []
    score_contradicao = 0.0
    
    # REGRA 1: Alta similaridade => n√£o contradiz
    if similaridade_semantica >= 0.55:
        return {
            "contradiz": False,
            "confianca": 0.0,
            "motivo": f"Alta similaridade sem√¢ntica ({similaridade_semantica:.2f}) indica confirma√ß√£o",
            "evidencias": []
        }
    
    # REGRA 2: N√∫meros similares => n√£o contradiz
    nums_original = extrair_numeros(texto_original)
    nums_fonte = extrair_numeros(texto_fonte)
    if nums_original and nums_fonte:
        if numeros_similares(nums_original, nums_fonte, tolerancia=0.05):
            return {
                "contradiz": False,
                "confianca": 0.0,
                "motivo": f"N√∫meros similares encontrados: {nums_original} ‚âà {nums_fonte}",
                "evidencias": []
            }
    
    # REGRA 3: Padr√µes fortes de desmentido
    for pattern in STRONG_DEBUNK_PATTERNS:
        match = re.search(pattern, texto_fonte_lower)
        if match:
            score_contradicao += 0.5
            evidencias.append(f"Padr√£o de desmentido: '{match.group(0)}'")
    
    # REGRA 4: Nega√ß√µes fortes pr√≥ximas de palavras-chave
    palavras_chave = [w for w in texto_original_lower.split() if len(w) > 4 and w not in NEUTRAL_WORDS][:5]
    negacoes_contextuais = 0
    for palavra_chave in palavras_chave:
        posicoes = [m.start() for m in re.finditer(re.escape(palavra_chave), texto_fonte_lower)]
        for pos in posicoes:
            inicio = max(0, pos - 50)
            fim = min(len(texto_fonte_lower), pos + len(palavra_chave) + 50)
            contexto = texto_fonte_lower[inicio:fim]
            for negacao in STRONG_NEGATION_WORDS:
                if negacao in contexto:
                    negacoes_contextuais += 1
                    evidencias.append(f"Nega√ß√£o '{negacao}' pr√≥xima de '{palavra_chave}'")
    if negacoes_contextuais >= 2:
        score_contradicao += min(negacoes_contextuais * 0.15, 0.3)
    
    # REGRA 5: Similaridade m√©dia reduz score
    if similaridade_semantica >= 0.40:
        score_contradicao *= 0.3
        evidencias.append(f"Score reduzido devido √† similaridade moderada ({similaridade_semantica:.2f})")
    
    # Decis√£o final (limiar alto)
    contradiz = score_contradicao >= 0.6
    motivo = f"Contradi√ß√£o detectada (confian√ßa: {score_contradicao:.2f})" if contradiz else "Sem contradi√ß√£o clara detectada"
    
    return {
        "contradiz": contradiz,
        "confianca": min(score_contradicao, 1.0),
        "motivo": motivo,
        "evidencias": evidencias
    }


# ============================================================================
# CARREGAR MODELO (LAZY LOADING)
# ============================================================================

_semantic_model = None


def _carregar_modelo():
    """
    Carrega o modelo sentence-transformers apenas uma vez.
    """
    global _semantic_model
    
    if _semantic_model is None:
        print("üìö Carregando modelo sentence-transformers...")
        print(f"   Modelo: {Config.SENTENCE_TRANSFORMER_MODEL}")
        print("   ‚è≥ Isso pode demorar 30-60s na primeira vez...")
        
        inicio = time.time()
        try:
            _semantic_model = SentenceTransformer(Config.SENTENCE_TRANSFORMER_MODEL)
            tempo = time.time() - inicio
            print(f"   ‚úÖ Modelo carregado em {tempo:.1f}s!")
        except Exception as e:
            print(f"   ‚ùå ERRO ao carregar modelo: {e}")
            raise
    
    return _semantic_model


# ============================================================================
# CLASSE PRINCIPAL - SEMANTIC ANALYZER
# ============================================================================

class SemanticAnalyzer:
    """
    Analisador sem√¢ntico com detec√ß√£o INTELIGENTE de contradi√ß√µes.
    """
    def __init__(self):
        """Inicializa analyzer com modelo de IA"""
        self.model = _carregar_modelo()

    # === NOVO: gera√ß√£o de embedding com cache em disco ===
    def _gerar_embedding_com_cache(self, texto: str) -> np.ndarray:
        """
        Gera embedding com cache em disco. Usa hash do texto + id do modelo.
        Limita o texto a 2000 chars (mesma heur√≠stica do pipeline).
        """
        if not texto:
            return np.zeros((384,), dtype=np.float32)  # tamanho t√≠pico all-MiniLM; evita crash

        texto_lim = texto[:2000]

        # incluir modelo na chave para evitar colis√£o ao trocar de modelo
        model_id = getattr(Config, "SENTENCE_TRANSFORMER_MODEL", "default-model")
        key_src = f"{model_id}||{texto_lim}".encode("utf-8", "ignore")
        texto_hash = hashlib.md5(key_src).hexdigest()
        cache_path = os.path.join(CACHE_DIR, f"{texto_hash}.pkl")

        # cache hit
        if os.path.exists(cache_path):
            try:
                with open(cache_path, "rb") as f:
                    emb = pickle.load(f)
                if isinstance(emb, np.ndarray):
                    return emb
            except Exception:
                pass  # prossegue para rec√°lculo

        # gerar e salvar
        emb = self.model.encode(texto_lim, convert_to_numpy=True)
        try:
            with open(cache_path, "wb") as f:
                pickle.dump(emb, f, protocol=pickle.HIGHEST_PROTOCOL)
        except Exception:
            pass
        return emb

    def analisar_noticias(self, texto_original, conteudos_scraping):
        """
        Analisa todas as not√≠cias extra√≠das comparando com o texto original.
        """
        print(f"\nüî¨ Iniciando an√°lise sem√¢ntica INTELIGENTE...")
        
        # Gerar embedding do texto original (com cache)
        embedding_original = self._gerar_embedding_com_cache(texto_original)
        
        resultados_analise = {}
        total_analisados = 0
        confirmam_forte = 0
        confirmam_parcial = 0
        apenas_mencionam = 0
        nao_relacionados = 0
        contradizem = 0
        
        # Para cada fonte
        for fonte_nome, fonte_conteudos in conteudos_scraping.items():
            if fonte_nome == 'metadata':
                continue
            
            if not fonte_conteudos:
                resultados_analise[fonte_nome] = []
                continue
            
            print(f"\n   üì∞ Analisando {fonte_nome}: {len(fonte_conteudos)} not√≠cia(s)")
            
            analises_fonte = []
            
            for conteudo in fonte_conteudos:
                if not conteudo['sucesso']:
                    analises_fonte.append({
                        **conteudo,
                        'similaridade': 0.0,
                        'status': 'erro_extracao',
                        'motivo': conteudo['erro'],
                        'contradiz': False,
                        'confianca_contradicao': 0.0
                    })
                    continue
                
                # Calcular similaridade primeiro (embedding com cache)
                embedding_fonte = self._gerar_embedding_com_cache(conteudo['texto'])
                similaridade = cosine_similarity(
                    embedding_original.reshape(1, -1),
                    embedding_fonte.reshape(1, -1)
                )[0][0]
                
                # ‚úÖ Detec√ß√£o INTELIGENTE de contradi√ß√£o (passa similaridade)
                contradicao = detectar_contradicao_inteligente(
                    texto_original, 
                    conteudo['texto'],
                    float(similaridade)
                )
                
                # Analisar status
                analise = self._analisar_similaridade(
                    float(similaridade),
                    conteudo,
                    contradicao
                )
                
                analises_fonte.append(analise)
                total_analisados += 1
                
                # Contabilizar status
                if analise['status'] == 'CONTRADIZ':
                    contradizem += 1
                elif analise['status'] == 'confirma_forte':
                    confirmam_forte += 1
                elif analise['status'] == 'confirma_parcial':
                    confirmam_parcial += 1
                elif analise['status'] == 'menciona':
                    apenas_mencionam += 1
                else:
                    nao_relacionados += 1
            
            resultados_analise[fonte_nome] = analises_fonte
            
            # Resumo
            sucessos = sum(1 for a in analises_fonte if a.get('similaridade', 0) > 0)
            contrad = sum(1 for a in analises_fonte if a.get('status') == 'CONTRADIZ')
            print(f"      ‚úÖ {sucessos} an√°lise(s) conclu√≠da(s)")
            if contrad > 0:
                print(f"      ‚ö†Ô∏è  {contrad} contradi√ß√£o(√µes) detectada(s)")
        
        # Metadata final
        print(f"\n‚úÖ An√°lise sem√¢ntica conclu√≠da!")
        print(f"   Total analisado: {total_analisados}")
        if contradizem > 0:
            print(f"   ‚ö†Ô∏è  CONTRADIZEM: {contradizem}")
        print(f"   ‚úì Confirmam forte: {confirmam_forte}")
        print(f"   ‚úì Confirmam parcial: {confirmam_parcial}")
        print(f"   ~ Apenas mencionam: {apenas_mencionam}")
        print(f"   ‚úó N√£o relacionados: {nao_relacionados}")
        
        return {
            **resultados_analise,
            'metadata': {
                'total_analisados': total_analisados,
                'contradizem': contradizem,
                'confirmam_forte': confirmam_forte,
                'confirmam_parcial': confirmam_parcial,
                'apenas_mencionam': apenas_mencionam,
                'nao_relacionados': nao_relacionados
            }
        }
    
    
    def _analisar_similaridade(self, similaridade, conteudo, contradicao):
        """
        Classifica status baseado em similaridade e contradi√ß√£o.
        """
        if contradicao['contradiz'] and contradicao['confianca'] >= 0.7:
            status = 'CONTRADIZ'
            motivo = f"‚ö†Ô∏è {contradicao['motivo']}"
        elif similaridade >= Config.SIMILARITY_THRESHOLD_HIGH:
            status = 'confirma_forte'
            motivo = f'Alta similaridade ({similaridade:.1%}) - confirma informa√ß√£o'
        elif similaridade >= Config.SIMILARITY_THRESHOLD_MEDIUM:
            status = 'confirma_parcial'
            motivo = f'Similaridade moderada ({similaridade:.1%})'
        elif similaridade >= Config.SIMILARITY_THRESHOLD_LOW:
            status = 'menciona'
            motivo = f'Baixa similaridade ({similaridade:.1%}), apenas menciona'
        else:
            status = 'nao_relacionado'
            motivo = f'Similaridade muito baixa ({similaridade:.1%})'
        
        return {
            **conteudo,
            'similaridade': round(float(similaridade), 4),
            'status': status,
            'motivo': motivo,
            'contradiz': contradicao['contradiz'],
            'confianca_contradicao': contradicao['confianca'],
            'evidencias_contradicao': contradicao.get('evidencias', [])
        }
    
    
    # (mantido para compatibilidade, agora s√≥ redireciona ao m√©todo com cache)
    def _gerar_embedding(self, texto):
        """Compat: mant√©m a assinatura antiga, mas usa o cache novo."""
        return self._gerar_embedding_com_cache(texto)
    
    
    def comparar_dois_textos(self, texto1, texto2):
        """Compara dois textos."""
        emb1 = self._gerar_embedding_com_cache(texto1)
        emb2 = self._gerar_embedding_com_cache(texto2)
        
        similaridade = cosine_similarity(
            emb1.reshape(1, -1),
            emb2.reshape(1, -1)
        )[0][0]
        
        contradicao = detectar_contradicao_inteligente(texto1, texto2, float(similaridade))
        
        return {
            'similaridade': float(similaridade),
            'contradiz': contradicao['contradiz'],
            'confianca_contradicao': contradicao['confianca'],
            'motivo': contradicao['motivo'],
            'evidencias': contradicao.get('evidencias', [])
        }


# ============================================================================
# FUN√á√ÉO DE CONVENI√äNCIA
# ============================================================================

def analisar_semantica(texto_original, conteudos_scraping):
    """Fun√ß√£o simplificada para an√°lise sem√¢ntica."""
    analyzer = SemanticAnalyzer()
    return analyzer.analisar_noticias(texto_original, conteudos_scraping)


# ============================================================================
# TESTE DO M√ìDULO
# ============================================================================

if __name__ == "__main__":
    print("=" * 70)
    print("üß™ TESTANDO DETEC√á√ÉO INTELIGENTE DE CONTRADI√á√ÉO")
    print("=" * 70)
    print()
    
    analyzer = SemanticAnalyzer()
    
    # Teste 1: NOT√çCIA VERDADEIRA (n√£o deve contradizer)
    print("Teste 1: Not√≠cia VERDADEIRA sobre sal√°rio m√≠nimo")
    print("-" * 70)
    
    texto_real = "Governo Lula diz que quer sal√°rio m√≠nimo acima de R$ 1.631 em 2026"
    texto_fonte_real = "Governo Lula projeta sal√°rio m√≠nimo de R$ 1630 em 2026"
    
    resultado_real = analyzer.comparar_dois_textos(texto_real, texto_fonte_real)
    
    print(f"Original: {texto_real}")
    print(f"Fonte: {texto_fonte_real}")
    print(f"\n‚úÖ Similaridade: {resultado_real['similaridade']:.4f}")
    print(f"‚ùå Contradiz: {resultado_real['contradiz']}")
    print(f"üìä Confian√ßa: {resultado_real['confianca_contradicao']:.2f}")
    print(f"üí¨ Motivo: {resultado_real['motivo']}")
    if resultado_real['evidencias']:
        print(f"üîç Evid√™ncias: {resultado_real['evidencias']}")
    print()
    
    # Teste 2: FAKE NEWS (deve contradizer)
    print("Teste 2: FAKE NEWS sobre vacinas")
    print("-" * 70)
    
    texto_fake = "Vacinas contra COVID-19 matam instantaneamente"
    texto_fonte_fake = "√â falso que vacinas matam. N√£o h√° evid√™ncias cient√≠ficas"
    
    resultado_fake = analyzer.comparar_dois_textos(texto_fake, texto_fonte_fake)
    
    print(f"Original: {texto_fake}")
    print(f"Fonte: {texto_fonte_fake}")
    print(f"\n‚úÖ Similaridade: {resultado_fake['similaridade']:.4f}")
    print(f"‚ùå Contradiz: {resultado_fake['contradiz']}")
    print(f"üìä Confian√ßa: {resultado_fake['confianca_contradicao']:.2f}")
    print(f"üí¨ Motivo: {resultado_fake['motivo']}")
    if resultado_fake['evidencias']:
        print(f"üîç Evid√™ncias: {resultado_fake['evidencias']}")
    
    print()
    print("=" * 70)
    print("‚úÖ TESTES CONCLU√çDOS!")
    print("=" * 70)
